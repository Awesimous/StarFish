{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1beee379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:53:58.755373Z",
     "start_time": "2021-06-03T15:53:58.631617Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class YTstats:\n",
    "\n",
    "    def __init__(self, api_key, channel_id):\n",
    "        self.api_key = api_key\n",
    "        self.channel_id = channel_id\n",
    "        self.channel_statistics = None\n",
    "        self.video_data = None\n",
    "\n",
    "    def extract_all(self):\n",
    "        self.get_channel_statistics()\n",
    "        self.get_channel_video_data()\n",
    "\n",
    "    def get_channel_statistics(self):\n",
    "        \"\"\"Extract the channel statistics\"\"\"\n",
    "        print('get channel statistics...')\n",
    "        url = f'https://www.googleapis.com/youtube/v3/channels?part=statistics&id={self.channel_id}&key={self.api_key}'\n",
    "        pbar = tqdm(total=1)\n",
    "        \n",
    "        json_url = requests.get(url)\n",
    "        data = json.loads(json_url.text)\n",
    "        try:\n",
    "            data = data['items'][0]['statistics']\n",
    "        except KeyError:\n",
    "            print('Could not get channel statistics')\n",
    "            data = {}\n",
    "\n",
    "        self.channel_statistics = data\n",
    "        pbar.update()\n",
    "        pbar.close()\n",
    "        return data\n",
    "\n",
    "    def get_channel_video_data(self):\n",
    "        \"Extract all video information of the channel\"\n",
    "        print('get video data...')\n",
    "        channel_videos, channel_playlists = self._get_channel_content(limit=50)\n",
    "\n",
    "        parts=[\"snippet\", \"statistics\",\"contentDetails\", \"topicDetails\"]\n",
    "        for video_id in tqdm(channel_videos):\n",
    "            for part in parts:\n",
    "                data = self._get_single_video_data(video_id, part)\n",
    "                channel_videos[video_id].update(data)\n",
    "\n",
    "        self.video_data = channel_videos\n",
    "        return channel_videos\n",
    "\n",
    "    def _get_single_video_data(self, video_id, part):\n",
    "        \"\"\"\n",
    "        Extract further information for a single video\n",
    "        parts can be: 'snippet', 'statistics', 'contentDetails', 'topicDetails'\n",
    "        \"\"\"\n",
    "\n",
    "        url = f\"https://www.googleapis.com/youtube/v3/videos?part={part}&id={video_id}&key={self.api_key}\"\n",
    "        json_url = requests.get(url)\n",
    "        data = json.loads(json_url.text)\n",
    "        try:\n",
    "            data = data['items'][0][part]\n",
    "        except KeyError as e:\n",
    "            print(f'Error! Could not get {part} part of data: \\n{data}')\n",
    "            data = dict()\n",
    "        return data\n",
    "\n",
    "    def _get_channel_content(self, limit=None, check_all_pages=True):\n",
    "        \"\"\"\n",
    "        Extract all videos and playlists, can check all available search pages\n",
    "        channel_videos = videoId: title, publishedAt\n",
    "        channel_playlists = playlistId: title, publishedAt\n",
    "        return channel_videos, channel_playlists\n",
    "        \"\"\"\n",
    "        url = f\"https://www.googleapis.com/youtube/v3/search?key={self.api_key}&channelId={self.channel_id}&part=snippet,id&order=date\"\n",
    "        if limit is not None and isinstance(limit, int):\n",
    "            url += \"&maxResults=\" + str(limit)\n",
    "\n",
    "        vid, pl, npt = self._get_channel_content_per_page(url)\n",
    "        idx = 0\n",
    "        while(check_all_pages and npt is not None and idx < 10):\n",
    "            nexturl = url + \"&pageToken=\" + npt\n",
    "            next_vid, next_pl, npt = self._get_channel_content_per_page(nexturl)\n",
    "            vid.update(next_vid)\n",
    "            pl.update(next_pl)\n",
    "            idx += 1\n",
    "\n",
    "        return vid, pl\n",
    "\n",
    "    def _get_channel_content_per_page(self, url):\n",
    "        \"\"\"\n",
    "        Extract all videos and playlists per page\n",
    "        return channel_videos, channel_playlists, nextPageToken\n",
    "        \"\"\"\n",
    "        json_url = requests.get(url)\n",
    "        data = json.loads(json_url.text)\n",
    "        channel_videos = dict()\n",
    "        channel_playlists = dict()\n",
    "        if 'items' not in data:\n",
    "            print('Error! Could not get correct channel data!\\n', data)\n",
    "            return channel_videos, channel_videos, None\n",
    "\n",
    "        nextPageToken = data.get(\"nextPageToken\", None)\n",
    "\n",
    "        item_data = data['items']\n",
    "        for item in item_data:\n",
    "            try:\n",
    "                kind = item['id']['kind']\n",
    "                published_at = item['snippet']['publishedAt']\n",
    "                title = item['snippet']['title']\n",
    "                if kind == 'youtube#video':\n",
    "                    video_id = item['id']['videoId']\n",
    "                    channel_videos[video_id] = {'publishedAt': published_at, 'title': title}\n",
    "                elif kind == 'youtube#playlist':\n",
    "                    playlist_id = item['id']['playlistId']\n",
    "                    channel_playlists[playlist_id] = {'publishedAt': published_at, 'title': title}\n",
    "            except KeyError as e:\n",
    "                print('Error! Could not extract data from item:\\n', item)\n",
    "\n",
    "        return channel_videos, channel_playlists, nextPageToken\n",
    "\n",
    "    def dump(self):\n",
    "        \"\"\"Dumps channel statistics and video data in a single json file\"\"\"\n",
    "        if self.channel_statistics is None or self.video_data is None:\n",
    "            print('data is missing!\\nCall get_channel_statistics() and get_channel_video_data() first!')\n",
    "            return\n",
    "\n",
    "        fused_data = {self.channel_id: {\"channel_statistics\": self.channel_statistics,\n",
    "                              \"video_data\": self.video_data}}\n",
    "\n",
    "        channel_title = self.video_data.popitem()[1].get('channelTitle', self.channel_id)\n",
    "        channel_title = channel_title.replace(\" \", \"_\").lower()\n",
    "        filename = channel_title + '.json'\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(fused_data, f, indent=4)\n",
    "        \n",
    "        print('file dumped to', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876984f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:54:05.139494Z",
     "start_time": "2021-06-03T15:54:05.136808Z"
    }
   },
   "outputs": [],
   "source": [
    "API_KEY = ''\n",
    "channel_id = 'UCKbbjKo0BSaNB99FcY9bSPQ'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd90c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract channel info and all videos with detailed info\n",
    "yt = YTstats(API_KEY, channel_id)\n",
    "yt.extract_all()\n",
    "yt.dump()  # dumps to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b6c9360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T21:21:07.522978Z",
     "start_time": "2021-06-02T21:21:07.339814Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get channel statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'viewCount': '108657354',\n",
       " 'subscriberCount': '196000',\n",
       " 'hiddenSubscriberCount': False,\n",
       " 'videoCount': '1020'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only channel info\n",
    "yt.get_channel_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a636df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
